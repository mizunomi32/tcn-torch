{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tcn import TemporalConvNet\n",
    "import time\n",
    "import os\n",
    "import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.environ['MYDATA_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = data.Data(wd=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_x, raw_y = data_c.data(class_num=19, data_num=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "seq_len = 125\n",
    "epochs = 50\n",
    "iters = 100\n",
    "T = 130\n",
    "n_steps = T + (2 * seq_len)\n",
    "n_classes = 10  # Digits 0 - 9\n",
    "n_train = 10000\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 125, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(raw_x, raw_y, test_size=0.90, random_state=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train\n",
    "y_train_new = y_train\n",
    "for _ in range(6):\n",
    "    for i,j in zip(X_train,y_train):\n",
    "        X_train_new = np.append(X_train_new,i+np.random.normal(0, 1, (125, 3)).reshape(1,125,3))\n",
    "        y_train_new = np.append(y_train_new, j)\n",
    "X_train = X_train_new.reshape(-1,125,3)\n",
    "y_train = y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mytcn(nn.Module):\n",
    "    def __init__(self, num_inputs,num_outputs ,num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(Mytcn, self).__init__()\n",
    "        self.tcn = TemporalConvNet(num_inputs=num_inputs, num_channels=num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], num_outputs)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.tcn(x)\n",
    "        \n",
    "        return self.linear(y[:, :, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chans = [16] * (4) \n",
    "model = Mytcn(num_inputs=raw_x.shape[1],num_outputs=19, num_channels=num_chans, kernel_size=6, dropout=0.1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "lr = 0.001\n",
    "optimizer =optim.RMSprop(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "correct = 0\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.5706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "100 tensor(1.5642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "200 tensor(1.5629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "300 tensor(1.5655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "400 tensor(1.5653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "500 tensor(1.5678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "600 tensor(1.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "700 tensor(1.5683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "800 tensor(1.5648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "900 tensor(1.5629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1000 tensor(1.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1100 tensor(1.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1200 tensor(1.5870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1300 tensor(1.5610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1400 tensor(1.5607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1500 tensor(1.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1600 tensor(1.5640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1700 tensor(1.5595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1800 tensor(1.5604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1900 tensor(1.5584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2000 tensor(1.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2100 tensor(1.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2200 tensor(1.5604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2300 tensor(1.5639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2400 tensor(1.5621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2500 tensor(1.5627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2600 tensor(1.5620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2700 tensor(1.5574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2800 tensor(1.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2900 tensor(1.5569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3000 tensor(1.5563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3100 tensor(1.5608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3200 tensor(1.5641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3300 tensor(1.5569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3400 tensor(1.6566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3500 tensor(1.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3600 tensor(1.5584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3700 tensor(1.5658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3800 tensor(1.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3900 tensor(1.5568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4000 tensor(1.5550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4100 tensor(1.5588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4200 tensor(1.5590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4300 tensor(1.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4400 tensor(1.5601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4500 tensor(1.5576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4600 tensor(1.5636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4700 tensor(1.5614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4800 tensor(1.5638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4900 tensor(1.5589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5000 tensor(1.6185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5100 tensor(1.5563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5200 tensor(1.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5300 tensor(1.5618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5400 tensor(1.5652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5500 tensor(1.5701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5600 tensor(1.5576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5700 tensor(1.5568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5800 tensor(1.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5900 tensor(1.5629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6000 tensor(1.5578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6100 tensor(1.5594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6200 tensor(1.5581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6300 tensor(1.5637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6400 tensor(1.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6500 tensor(1.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6600 tensor(1.5561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6700 tensor(1.5563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6800 tensor(1.5560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6900 tensor(1.5622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7000 tensor(1.5713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7100 tensor(1.5592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7200 tensor(1.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7300 tensor(1.5587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7400 tensor(1.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7500 tensor(1.5563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7600 tensor(1.5648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7700 tensor(1.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7800 tensor(1.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7900 tensor(1.5577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8000 tensor(1.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8100 tensor(1.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8200 tensor(1.5609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8300 tensor(1.5592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8400 tensor(1.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8500 tensor(1.5596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8600 tensor(1.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8700 tensor(1.5613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8800 tensor(1.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8900 tensor(1.5610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9000 tensor(1.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9100 tensor(1.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9200 tensor(1.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9300 tensor(1.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9400 tensor(1.5550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9500 tensor(1.5562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9600 tensor(1.5555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9700 tensor(1.5541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9800 tensor(1.5538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9900 tensor(1.5532, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor(X_train,dtype=torch.float32).cuda()\n",
    "labels = torch.tensor(y_train,dtype=torch.long).cuda()\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    out= model(input)\n",
    "    loss = criterion(out, labels)\n",
    "    if i%100==0:\n",
    "#         input = torch.tensor([X_train[1]],dtype=torch.float32)\n",
    "#         out= model(input)\n",
    "#         print(out)\n",
    "#         print(torch.max(out, 1))\n",
    "        print(i,loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25548245614035087"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(X_test,dtype=torch.float32).cuda()\n",
    "out= model(input)\n",
    "\n",
    "a,b =torch.max(out, 1)\n",
    "l=torch.tensor(y_test,dtype=torch.long).cuda()\n",
    "acc=(b == l).sum().item()/len(b)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45958646616541354"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(X_train,dtype=torch.float32).cuda()\n",
    "out= model(input)\n",
    "\n",
    "a,b =torch.max(out, 1)\n",
    "l=torch.tensor(y_train,dtype=torch.long).cuda()\n",
    "acc=(b == l).sum().item()/len(b)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
