{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tcn import TemporalConvNet\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.environ['MYDATA_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = data.Data(wd=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_x, raw_y = data_c.data(class_num=19, data_num=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 125, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "seq_len = 125\n",
    "epochs = 50\n",
    "iters = 100\n",
    "T = 130\n",
    "n_steps = T + (2 * seq_len)\n",
    "n_classes = 10  # Digits 0 - 9\n",
    "n_train = 10000\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 125, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(raw_x, raw_y, test_size=0.90, random_state=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train\n",
    "y_train_new = y_train\n",
    "for i,j in zip(X_train,y_train):\n",
    "    X_train_new = np.append(X_train_new,i+np.random.normal(0, 1, (125, 3)).reshape(1,125,3))\n",
    "    y_train_new = np.append(y_train_new, j)\n",
    "X_train = X_train_new.reshape(-1,125,3)\n",
    "y_train = y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mytcn(nn.Module):\n",
    "    def __init__(self, num_inputs,num_outputs ,num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(Mytcn, self).__init__()\n",
    "        self.tcn = TemporalConvNet(num_inputs=num_inputs, num_channels=num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], num_outputs)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(1, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.tcn(x)\n",
    "        \n",
    "#         return  self.linear(y.transpose(1, 2))\n",
    "        return F.softmax(self.linear(y[:, :, -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chans = [4] * (4) \n",
    "model = Mytcn(num_inputs=raw_x.shape[1],num_outputs=19, num_channels=num_chans, kernel_size=8, dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/mizuno/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 19])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(raw_x[0:100],dtype=torch.float32)\n",
    "model(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer =optim.RMSprop(model.parameters(),lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "correct = 0\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.99\n",
       "    centered: False\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/mizuno/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.9443, grad_fn=<NllLossBackward>)\n",
      "50 tensor(2.8324, grad_fn=<NllLossBackward>)\n",
      "100 tensor(2.7682, grad_fn=<NllLossBackward>)\n",
      "150 tensor(2.7378, grad_fn=<NllLossBackward>)\n",
      "200 tensor(2.7179, grad_fn=<NllLossBackward>)\n",
      "250 tensor(2.7048, grad_fn=<NllLossBackward>)\n",
      "300 tensor(2.7007, grad_fn=<NllLossBackward>)\n",
      "350 tensor(2.6874, grad_fn=<NllLossBackward>)\n",
      "400 tensor(2.6810, grad_fn=<NllLossBackward>)\n",
      "450 tensor(2.6769, grad_fn=<NllLossBackward>)\n",
      "500 tensor(2.6747, grad_fn=<NllLossBackward>)\n",
      "550 tensor(2.6774, grad_fn=<NllLossBackward>)\n",
      "600 tensor(2.6674, grad_fn=<NllLossBackward>)\n",
      "650 tensor(2.6693, grad_fn=<NllLossBackward>)\n",
      "700 tensor(2.6634, grad_fn=<NllLossBackward>)\n",
      "750 tensor(2.6613, grad_fn=<NllLossBackward>)\n",
      "800 tensor(2.6602, grad_fn=<NllLossBackward>)\n",
      "850 tensor(2.6570, grad_fn=<NllLossBackward>)\n",
      "900 tensor(2.6572, grad_fn=<NllLossBackward>)\n",
      "950 tensor(2.6544, grad_fn=<NllLossBackward>)\n",
      "1000 tensor(2.6524, grad_fn=<NllLossBackward>)\n",
      "1050 tensor(2.6516, grad_fn=<NllLossBackward>)\n",
      "1100 tensor(2.6480, grad_fn=<NllLossBackward>)\n",
      "1150 tensor(2.6481, grad_fn=<NllLossBackward>)\n",
      "1200 tensor(2.6406, grad_fn=<NllLossBackward>)\n",
      "1250 tensor(2.6439, grad_fn=<NllLossBackward>)\n",
      "1300 tensor(2.6414, grad_fn=<NllLossBackward>)\n",
      "1350 tensor(2.6377, grad_fn=<NllLossBackward>)\n",
      "1400 tensor(2.6358, grad_fn=<NllLossBackward>)\n",
      "1450 tensor(2.6336, grad_fn=<NllLossBackward>)\n",
      "1500 tensor(2.6352, grad_fn=<NllLossBackward>)\n",
      "1550 tensor(2.6341, grad_fn=<NllLossBackward>)\n",
      "1600 tensor(2.6312, grad_fn=<NllLossBackward>)\n",
      "1650 tensor(2.6347, grad_fn=<NllLossBackward>)\n",
      "1700 tensor(2.6418, grad_fn=<NllLossBackward>)\n",
      "1750 tensor(2.6361, grad_fn=<NllLossBackward>)\n",
      "1800 tensor(2.6270, grad_fn=<NllLossBackward>)\n",
      "1850 tensor(2.6264, grad_fn=<NllLossBackward>)\n",
      "1900 tensor(2.6231, grad_fn=<NllLossBackward>)\n",
      "1950 tensor(2.6168, grad_fn=<NllLossBackward>)\n",
      "2000 tensor(2.6113, grad_fn=<NllLossBackward>)\n",
      "2050 tensor(2.6154, grad_fn=<NllLossBackward>)\n",
      "2100 tensor(2.6168, grad_fn=<NllLossBackward>)\n",
      "2150 tensor(2.6067, grad_fn=<NllLossBackward>)\n",
      "2200 tensor(2.5994, grad_fn=<NllLossBackward>)\n",
      "2250 tensor(2.5978, grad_fn=<NllLossBackward>)\n",
      "2300 tensor(2.5950, grad_fn=<NllLossBackward>)\n",
      "2350 tensor(2.6047, grad_fn=<NllLossBackward>)\n",
      "2400 tensor(2.6185, grad_fn=<NllLossBackward>)\n",
      "2450 tensor(2.5855, grad_fn=<NllLossBackward>)\n",
      "2500 tensor(2.6000, grad_fn=<NllLossBackward>)\n",
      "2550 tensor(2.5848, grad_fn=<NllLossBackward>)\n",
      "2600 tensor(2.5860, grad_fn=<NllLossBackward>)\n",
      "2650 tensor(2.5858, grad_fn=<NllLossBackward>)\n",
      "2700 tensor(2.5843, grad_fn=<NllLossBackward>)\n",
      "2750 tensor(2.5837, grad_fn=<NllLossBackward>)\n",
      "2800 tensor(2.5848, grad_fn=<NllLossBackward>)\n",
      "2850 tensor(2.5809, grad_fn=<NllLossBackward>)\n",
      "2900 tensor(2.5786, grad_fn=<NllLossBackward>)\n",
      "2950 tensor(2.5771, grad_fn=<NllLossBackward>)\n",
      "3000 tensor(2.6042, grad_fn=<NllLossBackward>)\n",
      "3050 tensor(2.5886, grad_fn=<NllLossBackward>)\n",
      "3100 tensor(2.5852, grad_fn=<NllLossBackward>)\n",
      "3150 tensor(2.5730, grad_fn=<NllLossBackward>)\n",
      "3200 tensor(2.5934, grad_fn=<NllLossBackward>)\n",
      "3250 tensor(2.5712, grad_fn=<NllLossBackward>)\n",
      "3300 tensor(2.5703, grad_fn=<NllLossBackward>)\n",
      "3350 tensor(2.5658, grad_fn=<NllLossBackward>)\n",
      "3400 tensor(2.6045, grad_fn=<NllLossBackward>)\n",
      "3450 tensor(2.5603, grad_fn=<NllLossBackward>)\n",
      "3500 tensor(2.5551, grad_fn=<NllLossBackward>)\n",
      "3550 tensor(2.5560, grad_fn=<NllLossBackward>)\n",
      "3600 tensor(2.5540, grad_fn=<NllLossBackward>)\n",
      "3650 tensor(2.5983, grad_fn=<NllLossBackward>)\n",
      "3700 tensor(2.5546, grad_fn=<NllLossBackward>)\n",
      "3750 tensor(2.5502, grad_fn=<NllLossBackward>)\n",
      "3800 tensor(2.5494, grad_fn=<NllLossBackward>)\n",
      "3850 tensor(2.5548, grad_fn=<NllLossBackward>)\n",
      "3900 tensor(2.5472, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-43625da014a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         print(torch.max(out, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input = torch.tensor(X_train,dtype=torch.float32)\n",
    "labels = torch.tensor(y_train,dtype=torch.long)\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    out= model(input)\n",
    "    loss = criterion(out, labels)\n",
    "    if i%50==0:\n",
    "#         input = torch.tensor([X_train[1]],dtype=torch.float32)\n",
    "#         out= model(input)\n",
    "#         print(out)\n",
    "#         print(torch.max(out, 1))\n",
    "        print(i,loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(X_test,dtype=torch.float32)\n",
    "out= model(input)\n",
    "\n",
    "a,b =torch.max(out, 1)\n",
    "l=torch.tensor(y_test,dtype=torch.long)\n",
    "acc=(b == l).sum().item()/len(b)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(X_train,dtype=torch.float32)\n",
    "out= model(input)\n",
    "\n",
    "a,b =torch.max(out, 1)\n",
    "l=torch.tensor(y_train,dtype=torch.long)\n",
    "acc=(b == l).sum().item()/len(b)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
